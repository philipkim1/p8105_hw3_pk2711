---
title: "Homework 3"
author: "Philip Kim"
date: "10/12/2021"
output: 
  github_document:
    toc: true
    toc_depth: 6
---

```{r, message = FALSE}
library(tidyverse)
library(knitr)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

# Problem 1

```{r}
data("instacart")
```

## Dataset description

The instacart dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. This dataset provides information on orders through Instacart via identifiers such as `order_id`, `product_id`, `aisle_id` and etc. Accordingly, it gives in formation on the time of the order such as the day of the week the order was placed, `order_dow`, and the hour of the day, `order_hour_of_day`. Through this data, we can see that most users tend to buy products that they had purchased in the past, which was represented by a `r median(pull(instacart, reordered))` in the dataset. Correspondingly, the largest number of products ordered by a single user was `r max(pull(instacart, add_to_cart_order))`. 

## Number of Aisles/Most Ordered from Aisles

```{r, message = FALSE}
aisle_df = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs))
```

There are `r nrow(aisle_df)` aisles and the aisles with the most items ordered from are the fresh vegetables and the fresh fruits aisles with 150,609 and 150,473 orders, respectively. These two aisles have almost double the amount of orders from the next leading aisle, which is quite a big margin.

## Aisle Plot

```{r, message = FALSE}
aisle_df %>% 
  filter(n_obs > 10000) %>% 
  mutate(
    aisle = as.factor(aisle),
    aisle = fct_reorder(aisle, (n_obs))) %>% 
  ggplot((aes(x = aisle, y = n_obs))) + 
  labs(
    title = "Item Popularity in Instacart",
    x = "Aisle Name",
    y = "Number of Ordered Items") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_col() +
  coord_flip()
```

As the graph shows, the fresh vegetables and fresh fruits are the most popular items by a large margin which is then followed by packaged vegetable fruits and yogurt. Towards the bottom of the list of items, with more than 10,000 orders, include butter, oils vinegars, and dry pasta. However, the differences aren't quite as large as we see between the items at the top of the list.


## Most Popular Item Table

```{r, message = FALSE}
instacart %>% 
  filter(
    aisle == "baking ingredients" | 
    aisle == "dog food care" | 
    aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  mutate(rank = rank(-n_obs)) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  relocate(-n_obs) %>% 
  kable(caption = "Most popular items in Baking Ingredients, Dog Food Care, and Packaged Vegetables Fruits")
```

Organic Baby Spinach was the most popular packaged vegetable item, whereas Light Brown Sugar was the most popular item in baking ingredients. Accordingly, Snack Sticks Chicken & Rice Recipe Dog Treats was the most popular item in dog food care. Although these were the three most popular items in each aforementioned category, the number of orders in the most popular packaged vegetables fruits items markedly prevailed over the number of items in both baking ingredients and dog food care.  

## Pink Lady Apples/Coffee Ice Cream Table

```{r, message = FALSE}
instacart %>% 
  filter(product_name == "Pink Lady Apples" |
         product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    mean_hour = round(mean_hour, digits = 2),
    order_dow = recode(order_dow, "0" = "Sun",
                                  "1" = "Mon",
                                  "2" = "Tues",
                                  "3" = "Wed",
                                  "4" = "Thurs",
                                  "5" = "Fri",
                                  "6" = "Sat"),
    order_dow = as.factor(order_dow)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour) %>% 
  kable(caption = "Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week")
```

For the Coffee Ice Cream, the mean hour of day in which it is ordered is earlier in the weekends when compared to the weekdays. For the Pink Lady Apples, the mean hour of day that it is ordered is the latest on Wednesday. When comparing the two items, the Coffee Ice Cream is typically ordered later than the Pink Lady Apples.   

# Problem 2

```{r}
data("brfss_smart2010")
```

## Data Cleaning

```{r, message = FALSE}
brfss_tidy_smart2010 =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response == "Excellent" |
         response == "Very good" |
         response == "Good" |
         response == "Fair" |   
         response == "Poor") %>% 
  mutate(response = as.factor(response),
         response = ordered(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```


## States observed at 7 or more locations

```{r, message = FALSE}
brfss_tidy_smart2010 %>% 
  filter(year == 2002 |
         year == 2010) %>%
  group_by(year, locationabbr) %>% 
  summarize(n_obs = n_distinct(locationdesc)) %>% 
  filter(n_obs >= 7) %>% 
  arrange(year, (n_obs)) %>% 
  kable()
```

There were a total of 6 states that were observed at 7 or more locations in 2002 which include CT, FL, MA, NC, NJ, and PA. Relatively, there were a total of 14 states that were observed at 7 or more locations in 2010 which include CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA. A total of 5 states were observed at 7 or more locations in both 2002 and 2010, which include FL, MA, NC, NJ, and PA. 

## Spaghetti Plot

```{r, message = FALSE}
brfss_tidy_smart2010 %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr) %>% 
  summarize(mean_data_value = mean(data_value, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_data_value)) +
  labs(
    x = "Year",
    y = "Average Data Value within State",
    title = "Average Data Value of those with \nExcellent Overall Health over Time by State") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_line(aes(group = locationabbr))
```

The lowest average data value is 11.5 and is from WV in 2005. Accordingly, four out of the five lowest average data values are from WV. To contrast, the highest average data value is from UT in 2002. There are no clear trends from this plot but it seems that the average data value from all states, 2002 to 2010, hover around the 20 to 25 mark. 

## Two-Panel Plot

```{r, message = FALSE}
brfss_tidy_smart2010 %>% 
  filter(year == 2006 |
         year == 2010,
         locationabbr == "NY") %>% 
  group_by(year, response) %>% 
  summarize(data_value = data_value) %>% 
  ggplot(aes(x = response, y = data_value)) +
  labs(
    x = "Response",
    y = "Data Value",
    title = "Distribution of Data Value by Response Type in 2006 and 2010") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_boxplot() +
  facet_grid(. ~ year)
```

The distribution of data value seems to be quite similar for both the years 2006 and 2010. The highest data values correspond to the responses `Good` and `Very good` whereas the lowest data values correlate to `Poor` for both 2006 and 2010.  

# Problem 3

## Load and Tidy Data

```{r, message = FALSE}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(
    weekend = if_else(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
    minute_of_day = as.numeric(minute_of_day),
    day = as.factor(day),
    day = ordered(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  relocate(week, day_id, weekend, everything())
```

There are `r nrow(accel_df)` rows and `r ncol(accel_df)` columns within this dataset. There is a specific `day_id` variable that differentiates the day from others and `minute_of_day` variable that lists the exact minute of the day. Accordingly, there is a `weekend` variable that states whether the given day is a weekday or weekend and an `activity_count` that measures the level of activity for any given minute. Throughout the five weeks, the lowest activity count measured was a value of `r min(pull(accel_df, activity_count))` whereas the highest activity count recorded had a value of `r max(pull(accel_df, activity_count))`, which was on Wednesday of the second week. 


## Total Activity per Day

```{r, message = FALSE}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity_count = sum(activity_count)) %>% 
  kable()
```

It seems that as it got closer to the later weeks, the activity count on the weekends started to decrease. Also, the days in the middle of the week seem to be the most consistent in terms of activity count (e.g., Tuesday, Wednesday, Thursday) whereas the other days seem to comparably dynamic.

## 24-hour activity time course plot

```{r, message = FALSE}
accel_df %>% 
  mutate(
    hour = minute_of_day %/% 60) %>% 
  group_by(day_id, day, hour) %>% 
  summarize(activity_count = sum(activity_count)) %>%
  ggplot(aes(x = hour, y = activity_count, color = day)) +
  labs(
    x = "Hour of the Day",
    y = "Activity Count",
    title = "24 Hour Activity Time Course per Day",
    color = "Day of the Week") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_line(aes(group = day_id))
```

From this graph, the highest activity counts seem to be most prominent at the 20th hour of the day, followed by spikes around the 10 hour mark. Most of the activity seems to start around the 5 hour mark and end towards the 23 hour mark. Accordingly, the hours that have the lowest activity were those around the 24 and 0 hour marks. The participant typically has the highest activity counts for the days of Monday and Friday. This may be due to the weekday starting/ending. 